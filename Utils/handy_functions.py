import torch
import torch.nn.functional as F

from IntervalNets.interval_modules import parse_logits

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os
import random
from typing import Tuple


def set_seed(value):
    """
    Set deterministic results according to the given value
    (including random, numpy, and torch libraries).

    Parameters:
    -----------
    value: int
        The seed value to set for reproducibility.
    """
    random.seed(value)
    np.random.seed(value)
    torch.manual_seed(value)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def append_row_to_file(filename, elements, header=""):
    """
    Append a single row to the given file.

    Parameters
    ----------
    filename: folder and name of file
    elements: elements to saving in filename
    """
    if not filename.endswith(".csv"):
        filename += ".csv"
    filename = filename.replace(".pt", "")
    with open(filename, "a+") as stream:
        np.savetxt(stream, np.array(elements)[np.newaxis], delimiter=";", fmt="%s", header=header)


def write_pickle_file(filename, object_to_save):
    """
    Append a single row to the given file.

    Parameters:
    -----------
    filename: str
        The path and name of the file.
    elements: List[str]
        Elements to be saved in the file.
    header: str, optional
        An optional header for the file.

    Returns:
    --------
    None
    """
    torch.save(object_to_save, f"{filename}.pt")


def load_pickle_file(filename):
    """
    Load an object saved with torch.save() from a file.

    Parameters:
    -----------
    filename: str
        The path and name of the file containing the pickled object.

    Returns:
    --------
    object
        The deserialized object loaded from the file.
    """
    return torch.load(filename, map_location=torch.device('cpu'))


def get_shapes_of_network(model):
    """
    Get the shape of all layers in the loaded model.

    Arguments:
    ----------
    model: torch.nn.Module
        An instance of a hypnettorch model, e.g., MLP from mnets.

    Returns:
    --------
    List[List[int]]
        A list of lists, where each inner list represents the shape of a consecutive
        network layer in the model.
    """
    shapes_of_model = []
    for layer in model.weights:
        shapes_of_model.append(list(layer.shape))
    return shapes_of_model

def reverse_predictions(target_network, tensor_input, lower_weights, middle_weights, 
                        upper_weights, condition=None):
    """
    Reverse predictions for lower, middle, and upper output of the
    target network

    Parameters:
    -----------
    target_network: object
        The target network for which predictions are computed.
    tensor_input: torch.Tensor
        The input tensor for which predictions are computed.
    lower_weights: torch.Tensor
        The lower weights generated by the hypernetwork.
    middle_weights: torch.Tensor
        The middle weights generated by the hypernetwork.
    upper_weights: torch.Tensor
        The upper weights generated by the hypernetwork.
    condition: object, optional
        Number of task which is used to apply batch normalization
        statistics to test set

    Returns:
    --------
    lower_pred: torch.Tensor
        Reverse predictions using the lower weights.
    middle_pred: torch.Tensor
        Reversed predictions using the middle weights.
    upper_pred: torch.Tensor
        Reversed predictions using the upper weights.
    """

    lower_pred = target_network.forward(x=tensor_input,
                                        weights=lower_weights,
                                        condition=condition)
    middle_pred = target_network.forward(x=tensor_input,
                                        weights=middle_weights,
                                        condition=condition)
    upper_pred = target_network.forward(x=tensor_input,
                                        weights=upper_weights,
                                        condition=condition)
    
    lower_pred, middle_pred = torch.minimum(lower_pred, middle_pred), torch.maximum(lower_pred, middle_pred)
    middle_pred, upper_pred = torch.minimum(middle_pred, upper_pred), torch.maximum(middle_pred, upper_pred)
    
    return lower_pred, middle_pred, upper_pred

def intersection_of_embeds(z_l: torch.Tensor, z_u: torch.Tensor) -> Tuple[torch.Tensor]:
    """
    Compute the intersection of lower and upper embedding bounds for each task.

    Parameters:
    -----------
    z_l: torch.Tensor
        Lower embedding bounds for each task, shape: [num_tasks x embed_dim].
    z_u: torch.Tensor
        Upper embedding bounds for each task, shape: [num_tasks x embed_dim].

    Returns:
    --------
    Tuple[torch.Tensor]
        A tuple containing:
        - z_l_common_embed: Lower bound intersection over task embeddings, shape: 1 x embed_dim.
        - z_u_common_embed: Upper bound intersection over task embeddings, shape: 1 x embed_dim.
    """

    z_l_max = z_l.max(dim=0).values
    z_u_min = z_u.min(dim=0).values

    z_u_common_embed = torch.where(z_u_min < z_l_max, (z_u_min + z_l_max)/2, z_u_min)
    z_l_common_embed = torch.where(z_u_min < z_l_max, (z_u_min + z_l_max)/2, z_l_max)

    assert (z_l_common_embed <= z_u_common_embed).all(), f"Upper bounds should be greater or equal to lower bounds"

    return z_l_common_embed, z_u_common_embed

def calculate_interval_intersection(hypernetwork, parameters, current_task_id):
    """
    Calculate the intersection of intervals.

    Parameters:
    -----------
    hypernetwork: hypenettorch.hnets.module
        A hypernetwork that generates weights for the target network.
    parameters: dictionary
        A dictionary with hyperparameters.
    current_task_id: int
        The current task ID.

    Returns:
    --------
    Tuple[torch.Tensor]
        A tuple with lower, middle, and upper logits of the intersection, when
        the current task ID is greater than one. Otherwise, only the middle
        of the interval is returned.
    """

    assert isinstance(current_task_id, int)

    with torch.no_grad():

        eps    = parameters["perturbated_epsilon"]
        n_embs = parameters["embedding_size"]
        sigma  = 0.5 * eps / n_embs

        if current_task_id == 0:
            first_emb = sigma * torch.cos(hypernetwork.conditional_params[0])

            return first_emb
        else:
            
            radii = torch.stack([
                eps * F.softmax(hypernetwork.perturbated_eps_T[i], dim=-1) for i in range(current_task_id+1)
            ], dim=0)

            z_temp = torch.stack([
                sigma * torch.cos(hypernetwork.conditional_params[i]) for i in range(current_task_id+1)
            ], dim=0)
            
            zl_inter_emb, zu_inter_emb = intersection_of_embeds(z_temp - radii, z_temp + radii)

            middle_inter_emb = (zu_inter_emb + zl_inter_emb) / 2.0

            return zl_inter_emb, middle_inter_emb, zu_inter_emb


def calculate_number_of_iterations(number_of_samples,
                                   batch_size,
                                   number_of_epochs):
    """
    Calculate the total number of iterations based on the number
    of samples, desired batch size, and number of training epochs.

    Parameters:
    -----------
    number_of_samples: int
        The number of individual samples.
    batch_size: int
        The number of samples entering the network at one iteration.
    number_of_epochs: int
        The desired number of training epochs.

    Returns:
    --------
    Tuple[int, int]
        A tuple containing:
        - no_of_iterations_per_epoch: The number of training iterations per one epoch.
        - total_no_of_iterations: The total number of training iterations.
    """
    no_of_iterations_per_epoch = int(
        np.ceil(number_of_samples / batch_size)
    )
    total_no_of_iterations = int(no_of_iterations_per_epoch * number_of_epochs)

    return no_of_iterations_per_epoch, total_no_of_iterations

def calculate_accuracy(data,
                       target_network,
                       lower_weights,
                       middle_weights,
                       upper_weights,
                       parameters,
                       evaluation_dataset):
    """
    Calculate accuracy for a given dataset using a selected network
    and lower, middle and upper weights generated by the hypernetwork.

    Parameters:
    -----------
    data: object
        An instance of the dataset (e.g., hypnettorch.data.special.permuted_mnist.PermutedMNIST)
        in the case of the PermutedMNIST dataset.
    target_network: object
        An instance of the target network.
    lower_weights: torch.nn.modules.container.ParameterList
        Lower weights for the target network.
    middle_weights: torch.nn.modules.container.ParameterList
        Middle weights for the target network.
    upper_weights: torch.nn.modules.container.ParameterList
        Upper weights for the target network.
    parameters: dict
        A dictionary containing the following keys:
        - "device": string, "cuda" or "cpu", defines in which device calculations
          will be performed.
        - "use_batch_norm_memory": bool, defines whether stored weights
          of the batch normalization layer should be used. If True, then
          "number_of_task" has to be given.
        - "number_of_task": int or None, gives information about which task is currently
          solved. The number must be given when "use_batch_norm_memory" is True.
        - "full_interval": bool, a flag to indicate whether the model is full interval
          or not.
    evaluation_dataset: string
        "validation" or "test"; defines whether a validation or a test set will be evaluated.

    Returns:
    --------
    torch.Tensor
        Accuracy for the selected setting.
    """
    assert (parameters["use_batch_norm_memory"] and
            parameters["number_of_task"] is not None) or \
           not parameters["use_batch_norm_memory"]
    assert evaluation_dataset in ["validation", "test"]
    target_network.eval()
    with torch.no_grad():
        # Currently results will be calculated on the validation or test set
        if evaluation_dataset == "validation":
            input_data = data.get_val_inputs()
            output_data = data.get_val_outputs()
            
        elif evaluation_dataset == "test":
            input_data = data.get_test_inputs()
            output_data = data.get_test_outputs()

        test_input = data.input_to_torch_tensor(
            input_data, parameters["device"], mode="inference"
        )
        test_output = data.output_to_torch_tensor(
            output_data, parameters["device"], mode="inference"
        )

        if parameters["use_batch_norm_memory"]:
            condition = parameters["number_of_task"]
        else:
            condition = None

        if parameters["full_interval"]:

            logits = target_network.forward(
                x=test_input,
                upper_weights=upper_weights,
                middle_weights=middle_weights,
                lower_weights=lower_weights,
                condition=condition
            )

            _, logits, _ = parse_logits(logits)
        
        else:
            _, logits, _ = reverse_predictions(target_network, 
                                               test_input, 
                                               lower_weights,
                                               middle_weights,
                                               upper_weights,
                                               condition)
            
        gt_classes = test_output.max(dim=1)[1]
            
        predictions = logits.max(dim=1)[1]
        accuracy = (torch.sum(gt_classes == predictions, dtype=torch.float32) /
                    gt_classes.numel()) * 100.
    return accuracy


def calculate_mse_loss(criterion,
                       data,
                       target_network,
                       lower_weights,
                       middle_weights,
                       upper_weights,
                       parameters,
                       evaluation_dataset):
    """
    Calculate MSE loss for a given dataset using a selected network
    and lower, middle and upper weights generated by the hypernetwork.

    Parameters:
    -----------
    criterion: nn.Module
        Implements a loss function (e.g., CrossEntropyLoss).
    data: object
        An instance of the dataset (e.g., hypnettorch.data.special.permuted_mnist.PermutedMNIST)
        in the case of the PermutedMNIST dataset.
    target_network: object
        An instance of the target network.
    lower_weights: torch.nn.modules.container.ParameterList
        Lower weights for the target network.
    middle_weights: torch.nn.modules.container.ParameterList
        Middle weights for the target network.
    upper_weights: torch.nn.modules.container.ParameterList
        Upper weights for the target network.
    parameters: dict
        A dictionary containing the following keys:
        - "device": string, "cuda" or "cpu", defines in which device calculations
          will be performed.
        - "use_batch_norm_memory": bool, defines whether stored weights
          of the batch normalization layer should be used. If True, then
          "number_of_task" has to be given.
        - "number_of_task": int or None, gives information about which task is currently
          solved. The number must be given when "use_batch_norm_memory" is True.
        - "full_interval": bool, a flag to indicate whether the model is full interval
          or not.
    evaluation_dataset: string
        "validation" or "test"; defines whether a validation or a test set will be evaluated.

    Returns:
    --------
    torch.Tensor
        Interval MSE loss for the selected setting.
    """
    assert (parameters["use_batch_norm_memory"] and
            parameters["number_of_task"] is not None) or \
           not parameters["use_batch_norm_memory"]
    assert evaluation_dataset in ["validation", "test"]
    target_network.eval()

    with torch.no_grad():
        # Currently results will be calculated on the validation or test set
        if evaluation_dataset == "validation":
            input_data = data.get_val_inputs()
            output_data = data.get_val_outputs()
            
        elif evaluation_dataset == "test":
            input_data = data.get_test_inputs()
            output_data = data.get_test_outputs()

        test_input = data.input_to_torch_tensor(
            input_data, parameters["device"], mode="inference"
        )
        test_output = data.output_to_torch_tensor(
            output_data, parameters["device"], mode="inference"
        )

        if parameters["use_batch_norm_memory"]:
            condition = parameters["number_of_task"]
        else:
            condition = None

        if parameters["full_interval"]:

            logits = target_network.forward(
                x=test_input,
                upper_weights=upper_weights,
                middle_weights=middle_weights,
                lower_weights=lower_weights,
                condition=condition
            )

            z_l, _, z_u = parse_logits(logits)
        
        else:
            z_l, _, z_u = reverse_predictions(target_network, 
                                               test_input, 
                                               lower_weights,
                                               middle_weights,
                                               upper_weights,
                                               condition)
        
        mse_loss = criterion(z_l, z_u, test_output)
       
    return mse_loss

def evaluate_previous_tasks_for_intersection(hypernetwork,
                            target_network,
                            universal_emb,
                            dataframe_results,
                            list_of_permutations,
                            parameters):
    """
    Evaluate the target network according to the weights generated
    by the hypernetwork for all previously trained tasks for the intersection
    of tasks' embeddings.

    Parameters:
    -----------
    hypernetwork: hypnettorch.hnets module
        A hypernetwork that generates weights for the target network.
    target_network: hypnettorch.mnets module
        A target network that will perform classification.
    universal_emb: torch.Tensor
        An embedding produced as the middle of intervals' intersection for already learned tasks.
    dataframe_results: Pandas DataFrame
        Stores results; contains the following columns: "after_learning_of_task",
        "tested_task", and "accuracy".
    list_of_permutations: hypnettorch.data module
        A list of permutations (e.g., in the case of PermutedMNIST, it will be
        special.permuted_mnist.PermutedMNISTList).
    parameters: dictionary
        A dictionary containing the following keys:
        - "device": string, "cuda" or "cpu", defines in which device calculations
          will be performed.
        - "use_batch_norm_memory": Boolean, defines whether stored weights
          of the batch normalization layer should be used. If True, then
          "number_of_task" has to be given.
        - "number_of_task": int/None, gives information about which task is currently
          solved.
        - "full_interval": bool, a flag to indicate whether we have full intervals
          or not.

    Returns:
    --------
    Pandas DataFrame
        A dataframe updated with the calculated results.
    """
    # Calculate accuracy for each previously trained task
    # as well as for the last trained task
    hypernetwork.eval()
    target_network.eval()

    inter_lower_weights, inter_target_weights, inter_upper_weights, _ = hypernetwork.forward(cond_input=universal_emb.view(1, -1),
                                                                        perturbated_eps=parameters["perturbated_epsilon"],
                                                                        return_extended_output=True,
                                                                        universal_emb=True)

    for task in range(parameters["number_of_task"] + 1):
        # Target entropy calculation should be included here: hypernetwork has to be inferred
        # for each task (together with the target network) and the task_id with the lowest entropy
        # has to be chosen
        # Arguments of the function: list of permutations, hypernetwork, sparsity, target network
        # output: task id
        currently_tested_task = list_of_permutations[task]
        
        accuracy = calculate_accuracy(
            currently_tested_task,
            target_network,
            inter_lower_weights,
            inter_target_weights,
            inter_upper_weights,
            parameters=parameters,
            evaluation_dataset="test"
        )
        result = {
            "after_learning_of_task": parameters["number_of_task"],
            "tested_task": task,
            "accuracy": accuracy.cpu().item()
        }
        print(f"Accuracy for task {task}: {accuracy}%.")
        dataframe_results = dataframe_results.append(
            result, ignore_index=True)
    return dataframe_results

def evaluate_previous_classification_tasks(hypernetwork,
                            target_network,
                            dataframe_results,
                            list_of_permutations,
                            parameters):
    """
    Evaluate the target network according to the weights generated
    by the hypernetwork for all previously trained tasks. For instance,
    if current_task_no is equal to 5, then tasks 0, 1, 2, 3, 4, and 5
    will be evaluated.
    
    Parameters:
    ----------
    hypernetwork: hypnettorch.hnets module
        A hypernetwork that generates weights for the target network.
    target_network: hypnettorch.mnets module
        A target network that finally performs classification.
    dataframe_results: Pandas DataFrame
        Stores results; contains the following columns: "after_learning_of_task",
        "tested_task", and "accuracy".
    list_of_permutations: hypnettorch.data module
        E.g., in the case of PermutedMNIST, it will be special.permuted_mnist.PermutedMNISTList.
    parameters: dictionary
        A dictionary containing the following keys:
        - "device": string, "cuda" or "cpu", defines in which device calculations
          will be performed.
        - "use_batch_norm_memory": Boolean, defines whether stored weights
          of the batch normalization layer should be used. If True, then
          "number_of_task" has to be given.
        - "number_of_task": int/None, gives information about which task is currently
          solved.

    Returns:
    --------
    Pandas DataFrame
        A dataframe updated with the calculated results.
    """
    # Calculate accuracy for each previously trained task
    # as well as for the last trained task
    hypernetwork.eval()
    target_network.eval()

    # The case when we know task id during interference
    for task in range(parameters["number_of_task"] + 1):
        # Target entropy calculation should be included here: hypernetwork has to be inferred
        # for each task (together with the target network) and the task_id with the lowest entropy
        # has to be chosen
        # Arguments of the function: list of permutations, hypernetwork, target network
        # output: task id

        currently_tested_task = list_of_permutations[task]
        print(f"currently_tested_task: {currently_tested_task}")

        # Generate weights of the target network
        lower_weights, target_weights, upper_weights, _ = hypernetwork.forward(cond_id=task, perturbated_eps=parameters["perturbated_epsilon"],
                                                                            return_extended_output=True)
        accuracy = calculate_accuracy(
            currently_tested_task,
            target_network,
            lower_weights,
            target_weights,
            upper_weights,
            parameters=parameters,
            evaluation_dataset="test"
        )
        
        result = {
            "after_learning_of_task": parameters["number_of_task"],
            "tested_task": task,
            "accuracy": accuracy.cpu().item()
        }
        print(f"Accuracy for task {task}: {accuracy}%.")
        dataframe_results = dataframe_results.append(
            result, ignore_index=True)
        
    return dataframe_results


def evaluate_previous_regression_tasks(
                            criterion,
                            hypernetwork,
                            target_network,
                            dataframe_results,
                            list_of_permutations,
                            parameters):
    """
    Evaluate the target network according to the weights generated
    by the hypernetwork for all previously trained tasks. For instance,
    if current_task_no is equal to 5, then tasks 0, 1, 2, 3, 4, and 5
    will be evaluated.
    
    Parameters:
    ----------
    criterion: nn.Module
        Implements a loss function (e.g., CrossEntropyLoss).
    hypernetwork: hypnettorch.hnets module
        A hypernetwork that generates weights for the target network.
    target_network: hypnettorch.mnets module
        A target network that finally performs regression.
    dataframe_results: Pandas DataFrame
        Stores results; contains the following columns: "after_learning_of_task",
        "tested_task", and "accuracy".
    list_of_permutations: hypnettorch.data module
        E.g., in the case of PermutedMNIST, it will be special.permuted_mnist.PermutedMNISTList.
    parameters: dictionary
        A dictionary containing the following keys:
        - "device": string, "cuda" or "cpu", defines in which device calculations
          will be performed.
        - "use_batch_norm_memory": Boolean, defines whether stored weights
          of the batch normalization layer should be used. If True, then
          "number_of_task" has to be given.
        - "number_of_task": int/None, gives information about which task is currently
          solved.

    Returns:
    --------
    Pandas DataFrame
        A dataframe updated with the calculated results.
    """
    # Calculate accuracy for each previously trained task
    # as well as for the last trained task
    hypernetwork.eval()
    target_network.eval()

    # The case when we know task id during interference
    for task in range(parameters["number_of_task"] + 1):

        currently_tested_task = list_of_permutations[task]
        print(f"currently_tested_task: {currently_tested_task}")

        # Generate weights of the target network
        lower_weights, target_weights, upper_weights, _ = hypernetwork.forward(cond_id=task, perturbated_eps=parameters["perturbated_epsilon"],
                                                                            return_extended_output=True)
        mse_loss = calculate_mse_loss(
            criterion,
            currently_tested_task,
            target_network,
            lower_weights,
            target_weights,
            upper_weights,
            parameters=parameters,
            evaluation_dataset="test"
        )
        
        result = {
            "after_learning_of_task": parameters["number_of_task"],
            "tested_task": task,
            "mse_loss": mse_loss.cpu().item()
        }
        print(f"MSE loss for task {task}: {mse_loss}.")
        dataframe_results = dataframe_results.append(
            result, ignore_index=True)
        
    return dataframe_results


def save_parameters(saving_folder,
                    parameters,
                    name=None):
    """
    Save hyperparameters to the selected file.

    Parameters:
    ----------
    saving_folder: str
        Defines a path to the folder for saving.
    parameters: dict
        Contains all hyperparameters to be saved.
    name: str, optional
        Name of the file for saving. If not provided, a timestamp-based name
        will be generated.
    """
    if name is None:
        current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        name = f"parameters_{current_time}.csv"
    with open(f"{saving_folder}/{name}", "w") as file:
        for key in parameters.keys():
            file.write(f"{key};{parameters[key]}\n")


def plot_heatmap(load_path):
    """
    Save hyperparameters to the selected file.

    Parameters:
    ----------
    saving_folder: str
        Defines a path to the folder for saving.
    parameters: dict
        Contains all hyperparameters to be saved.
    name: str, optional
        Name of the file for saving. If not provided, a timestamp-based name
        will be generated.
    """
    dataframe = pd.read_csv(load_path, delimiter=";", index_col=0)
    dataframe = dataframe.astype(
        {"after_learning_of_task": "int32",
         "tested_task": "int32"}
        )
    table = dataframe.pivot(
        "after_learning_of_task", "tested_task", "accuracy")
    sns.heatmap(table, annot=True, fmt=".1f")
    plt.tight_layout()
    plt.savefig(load_path.replace(".csv", ".pdf"), dpi=300)
    plt.close()

def plot_intervals_around_embeddings(hypernetwork,
                                     parameters,
                                     save_folder,
                                     iteration=None,
                                     current_task=None,
                                     plot_universal_embedding=None):
    """
    Plot intervals with trained radii around tasks' embeddings for
    all tasks at once.

    Parameters:
    ----------
    hypernetwork: nn.Module
        A hypernetwork instance.
    parameters: dict
        Contains necessary hyperparameters describing an experiment.
    save_folder: str
        Contains the folder where the plot will be saved.
    iteration: int, optional
        An iteration number.
    current_task: int, optional
        The current task ID.
    plot_universal_embedding: bool, optional
        A flag to add the intersection of embeddings to the plot.

    Returns:
    -------
        None
    """

    # Check if folder exists, if it doesn"t then create the folder
    if not os.path.exists(save_folder):
        os.mkdir(save_folder)

    no_tasks = current_task + 1 if current_task is not None else parameters["number_of_tasks"]
    n_embs   = parameters["embedding_size"]
    eps      = parameters["perturbated_epsilon"]

    with torch.no_grad():
        radii_params = hypernetwork.perturbated_eps_T
        cond_params  = hypernetwork.conditional_params

        embds_detached = [
            cond_params[i] for i in range(no_tasks)
        ]

        radii_detached = [
            eps * F.softmax(radii_params[i], dim=-1) for i in range(no_tasks)
        ]
        
        # Create a plot
        fig = plt.figure(figsize=(10, 6))
        cm  = plt.get_cmap("gist_rainbow")

        if not plot_universal_embedding:
            colors = [cm(1.*i/no_tasks) for i in range(no_tasks)]
        else:
            colors = [cm(1.*i/(no_tasks + 1)) for i in range(no_tasks + 1)]

        for task_id, (tasks_embeddings, radii_per_emb) in enumerate(zip(embds_detached, radii_detached)):
            
            tasks_embeddings = tasks_embeddings.cpu().detach().numpy()
            radii_per_emb = radii_per_emb.cpu().detach().numpy()

            # Generate an x axis
            x = [_ for _ in range(parameters["embedding_size"])]

            # Create a scatter plot
            plt.scatter(x, tasks_embeddings, label=f"Task_{task_id}", marker="o", c=[colors[task_id]], alpha=0.3)

            for i in range(len(x)):
                plt.vlines(x[i], ymin=tasks_embeddings[i] - radii_per_emb[i],
                            ymax=tasks_embeddings[i] + radii_per_emb[i], linewidth=2, colors=[colors[task_id]], alpha=0.3)
                
        if current_task is not None and \
            current_task > 0 and \
            plot_universal_embedding:

            zl_inter_emb, middle_inter_emb, zu_inter_emb = calculate_interval_intersection(hypernetwork=hypernetwork,
                                                                                            parameters=parameters,
                                                                                            current_task_id=current_task)
            
            radii = (zu_inter_emb - zl_inter_emb)/2.0

            middle_inter_emb = middle_inter_emb.cpu().detach().numpy()
            radii = radii.cpu().detach().numpy()
            
            plt.scatter(x, middle_inter_emb, label=f"Intersection", marker="o", c=[colors[-1]], alpha=1.0)

            for i in range(len(x)):
                plt.vlines(x[i], ymin=middle_inter_emb[i] - radii[i],
                            ymax=middle_inter_emb[i] + radii[i], linewidth=2, colors=[colors[-1]], alpha=1.0)


        # Create a save path
        if iteration is not None:
            save_path = f"{save_folder}/intervals_around_tasks_embeddings_task_{current_task}_{iteration}.png"
        else:
            save_path = f"{save_folder}/intervals_around_tasks_embeddings_final.png"

        # Add labels and a legend
        plt.xlabel("Embedding's coordinate")
        plt.ylabel("Embedding's value")
        plt.title(f'Intervals around embeddings with sum of radius = {parameters["perturbated_epsilon"]}, dim = {n_embs}')
        plt.xticks(x)
        plt.legend(loc="upper left", bbox_to_anchor=(1.05, 1.0))
        plt.grid()
        plt.tight_layout()
        plt.savefig(save_path, dpi=300)
        plt.close()